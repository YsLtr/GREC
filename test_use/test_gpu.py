# test_gpu_performance.py
"""
TensorFlow GPU æ€§èƒ½æµ‹è¯•
"""

import os
os.add_dll_directory(r'C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin')
import time
import numpy as np
import tensorflow as tf

def print_header(title):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 70)
    print(f"ðŸ“Š {title}")
    print("=" * 70)

def main():
    print_header("TensorFlow 2.6.0 GPU æ€§èƒ½æµ‹è¯•")
    
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
    print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
    print(f"NumPyç‰ˆæœ¬: {np.__version__}")
    
    # GPUä¿¡æ¯
    print_header("GPUè®¾å¤‡ä¿¡æ¯")
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print(f"âœ… æ‰¾åˆ° {len(gpus)} ä¸ªGPUè®¾å¤‡:")
        for i, gpu in enumerate(gpus):
            print(f"  GPU {i}: {gpu}")
            
            # èŽ·å–GPUè¯¦ç»†ä¿¡æ¯
            try:
                details = tf.config.experimental.get_device_details(gpu)
                if details:
                    for key, value in details.items():
                        print(f"    {key}: {value}")
            except:
                pass
    else:
        print("âŒ æœªæ‰¾åˆ°GPUè®¾å¤‡")
        return
    
    # æ€§èƒ½æµ‹è¯•
    print_header("GPUæ€§èƒ½æµ‹è¯•")
    
    test_sizes = [
        (100, 100, "å°çŸ©é˜µ"),
        (1000, 1000, "ä¸­ç­‰çŸ©é˜µ"),
        (3000, 3000, "å¤§çŸ©é˜µ"),
    ]
    
    results = []
    
    for rows, cols, description in test_sizes:
        print(f"\næµ‹è¯•: {description} ({rows}x{cols})")
        
        # GPUæµ‹è¯•
        with tf.device('/GPU:0'):
            start = time.time()
            a = tf.random.normal([rows, cols])
            b = tf.random.normal([cols, rows])
            c = tf.matmul(a, b)
            result = tf.reduce_sum(c)
            gpu_time = time.time() - start
            gpu_result = result.numpy()
        
        # CPUæµ‹è¯•
        with tf.device('/CPU:0'):
            start = time.time()
            a_cpu = tf.random.normal([rows, cols])
            b_cpu = tf.random.normal([cols, rows])
            c_cpu = tf.matmul(a_cpu, b_cpu)
            result_cpu = tf.reduce_sum(c_cpu)
            cpu_time = time.time() - start
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup = cpu_time / gpu_time if gpu_time > 0 else 0
        
        print(f"  GPUæ—¶é—´: {gpu_time:.3f}ç§’")
        print(f"  CPUæ—¶é—´: {cpu_time:.3f}ç§’")
        print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        results.append({
            'size': f"{rows}x{cols}",
            'gpu_time': gpu_time,
            'cpu_time': cpu_time,
            'speedup': speedup
        })
    
    # æ€»ç»“
    print_header("æ€§èƒ½æµ‹è¯•æ€»ç»“")
    print(f"{'æµ‹è¯•':<15} {'GPUæ—¶é—´(ç§’)':<15} {'CPUæ—¶é—´(ç§’)':<15} {'åŠ é€Ÿæ¯”':<10}")
    print("-" * 60)
    
    for result in results:
        print(f"{result['size']:<15} {result['gpu_time']:<15.3f} {result['cpu_time']:<15.3f} {result['speedup']:<10.2f}x")
    
    # è¿è¡Œæ‚¨çš„åŽŸå§‹æµ‹è¯•
    print_header("åŽŸå§‹é—®é¢˜æµ‹è¯•")
    print("è¿è¡Œ: tf.reduce_sum(tf.random.normal([1000, 1000]))")
    
    with tf.device('/GPU:0'):
        result = tf.reduce_sum(tf.random.normal([1000, 1000]))
        print(f"ç»“æžœ: {result.numpy():.6f}")
        print(f"è®¾å¤‡: {result.device}")
    
    print_header("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    import sys
    main()